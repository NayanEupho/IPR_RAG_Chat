# -*- coding: utf-8 -*-
"""RAG_Chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xbq20uY5za76_ZmauxzvJtiFJjji_4C3
"""

!pip install faiss-cpu langchain openai sentence-transformers PyMuPDF

"""Importing Libraries"""

!pip install -U langchain-community

import pymupdf  # for pdf text extraction
import re # regual expression
from langchain.vectorstores import FAISS # vector similarity search
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings # for emebedding generation of OpenAI and llama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os

from google.colab import files

uploaded = files.upload()
pdf_path = next(iter(uploaded))  # file path

def extract_text_from_pdf(pdf_path):
    doc = pymupdf.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return clean_text(text)

def clean_text(text):
    text = text.replace('\xa0', ' ')  # Non-breaking space
    text = re.sub(r'\n{2,}', '\n', text)  # Collapse newlines
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII
    text = re.sub(r' +', ' ', text)  # Collapse multiple spaces
    return text.strip()

text = extract_text_from_pdf(pdf_path)
print(text[:1000]) # cleaned raw text

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
docs = text_splitter.create_documents([text])
print(f"Total Chunks: {len(docs)} \n")

for i, doc in enumerate(docs):
  print(f"Chunk_{i + 1}: \n")
  print(f"{doc.page_content}\n")

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

db = FAISS.from_documents(docs, embedding)
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 1})

""" retriver test """

query = "Whats the conclusion ?"
results = retriever.get_relevant_documents(query)

for i, doc in enumerate(results):
    print(f"\nðŸ”¹ Match {i+1}")
    print(doc.page_content)

!pip install transformers accelerate torch

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # âœ… Chat version (for better RAG performance)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")

# Create generation pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.3,
    do_sample=False,
    top_p=0.95,
    repetition_penalty=1.1
)

# Wrap with LangChain
llm = HuggingFacePipeline(pipeline=pipe)

"""
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "tiiuae/falcon-rw-1b"  # Light and Colab-friendly

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

hf_pipe = pipeline("text-generation", model=model, tokenizer=tokenizer,
                   max_new_tokens=512, do_sample=True, temperature=0.7)

llm = HuggingFacePipeline(pipeline=hf_pipe)

"""

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

"""using default prompt/query template"""
#problem: Over answering
#hallucinating a full FAQ when using "tiiuae/falcon-rw-1b" but not when google large LLM

query = "What is the main topic of the document?"
result = qa_chain(query)

print("Answer: \n", result['result'])

print("\n Source Chunks Used:")
for i, doc in enumerate(result['source_documents']):
    print(f"\nðŸ”¹ Chunk {i+1}")
    print(doc.page_content)

llm.invoke("What is the capital of France?")

hf_pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=150,
    do_sample=False,       # <-- IMPORTANT
    temperature=0.3        # <-- Lower = more focused
)

from langchain.prompts import PromptTemplate

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a helpful assistant. Use the following document excerpt to answer the question as briefly and accurately as possible.

Context:
{context}

Question: {question}
Answer (brief and precise):"""
)


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs={"prompt": custom_prompt}
)

query = "where should I vist ?"
result = qa_chain(query)

print("Answer: \n", result['result'])

query = "what is the conclusion of this document ?"
result = qa_chain(query)

print("Answer: \n", result['result'])

print("\n Source Chunks Used:")
for i, doc in enumerate(result['source_documents']):
    print(f"\n* Chunk {i+1}")
    print(doc.page_content)

